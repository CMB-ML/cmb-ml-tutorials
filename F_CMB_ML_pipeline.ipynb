{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMB-ML Framework: Pipeline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMB-ML manages a complex pipeline that processes data across multiple stages. Each stage produces outputs that need to be tracked, reused, and processed in later stages. Without a clear framework, this can lead to disorganized code, redundant logic, and errors.\n",
    "\n",
    "The CMB-ML library provides a set of tools to manage these pipelines in a modular way. Each tool focuses on a specific task, such as handling data files, managing file paths, or defining pipeline stages. Together, they simplify building and maintaining complex workflows.\n",
    "\n",
    "In the [previous notebook](./E_CMB_ML_framework.ipynb), I built up to a single stage of a pipeline. In this notebook, I look at how the pipeline can be assembled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View this notebook with [nbviewer](https://nbviewer.org/github/CMB-ML/cmb-ml/tree/main/demonstrations/F_CMB_ML_pipeline.ipynb#Introduction) (or in your IDE) to enable these links.\n",
    "\n",
    "This notebook continues with the running [Example](#Example) from the previous notebook. It introduces two components of the CMB-ML library that exist outside the context of an individual stage, managing the whole pipeline instead:\n",
    "- [PipelineContext](#PipelineContext): Manages the Executors in a pipeline\n",
    "- [LogMaker](#LogMaker): Sets aside information used throughout the pipeline\n",
    "\n",
    "A short [Conclusion](#Conclusion) wraps up and provides a segue to the next notebook. This notebook is mercifully shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook continues with considering the same simple task: converting a power spectrum to a map.\n",
    "\n",
    "There are two stages. The first writes a power spectrum to the dataset as a text file. The second reads that power spectrum data and produces a \"CMB\" map.\n",
    "\n",
    "(In the previous notebook, you may not have noticed the first stage running, because it was hidden in the \"helper module.\" Almost all the code for this notebook is included.)\n",
    "\n",
    "I do recommend looking through the Set-Up section, as some minor changes have been made to the Executors.\n",
    "\n",
    "There is still a helper module, but it only contains import statements for use by the LogMaker object (the LogMaker was written with the assumption it would be called from a module)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the location of the data directory\n",
    "os.environ[\"CMB_ML_DATA\"] = \"/data/jim/CMB_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from hydra import compose, initialize\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "\n",
    "from cmbml.core import BaseStageExecutor, Asset\n",
    "from cmbml.core.asset_handlers import TextPowerSpectrum, HealpyMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a logger this time, instead of `print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"F_Tutorial\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Outside of a notebook, Hydra will handle the logging. \n",
    "handler = logging.StreamHandler()  # StreamHandler sends logs to sys.stdout by default\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the executors used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakePSExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, stage_str=\"ps_setup\")\n",
    "\n",
    "        self.out_cmb_ps: Asset = self.assets_out[\"cmb_ps\"]\n",
    "        # I note the handler for assets. When using an IDE, this makes\n",
    "        #   it easier to navigate to the handler's code.\n",
    "        out_cmb_ps_handler: TextPowerSpectrum\n",
    "\n",
    "    def execute(self):\n",
    "        ell = np.arange(200)\n",
    "        # This is a naive model for the CMB power spectrum.\n",
    "        #   It's just a polynomial fit up to ell=200 for the \n",
    "        #   Planck 2018 power spectrum.\n",
    "        cheap_model = [ 1.51935454e-13,\n",
    "                       -1.33044280e-10,\n",
    "                        4.87473463e-08,\n",
    "                       -9.68198860e-06,\n",
    "                        1.12257186e-03, \n",
    "                       -7.62816561e-02,\n",
    "                        3.00276536e+00, \n",
    "                       -4.49411282e+01,\n",
    "                        1.04893659e+03]\n",
    "        ps = np.poly1d(cheap_model)\n",
    "        self.out_cmb_ps.write(data=ps(ell))\n",
    "        logger.info(f\"CMB power spectrum written to {self.out_cmb_ps.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS2MapExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        super().__init__(cfg, stage_str=\"ps2map\")\n",
    "\n",
    "        self.out_map_asset = self.assets_out[\"cmb_map\"]\n",
    "        self.in_ps_asset = self.assets_in[\"cmb_ps\"]\n",
    "        # Note handlers for easy reference\n",
    "        out_map_handler: HealpyMap\n",
    "        in_ps_handler: TextPowerSpectrum\n",
    "\n",
    "        self.nside = cfg.scenario.nside\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        logger.info(f\"Power spectrum read from {self.in_ps_asset.path}\")\n",
    "        ps = self.in_ps_asset.read()\n",
    "        cmb = hp.synfast(ps, nside=self.nside)\n",
    "        self.out_map_asset.write(data=cmb)\n",
    "        logger.info(f\"Map written to {self.out_map_asset.path}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In a top-level script, I would instead have something like:\n",
    "\n",
    "```python\n",
    "from phase_dir.stage_executors import (\n",
    "    MakePSExecutor,\n",
    "    PS2MapExecutor\n",
    "    )\n",
    "```\n",
    "\n",
    "Here, I just include the code. Take a look at [the top-level simulation script](../main_sims.py) for an example of these imports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "# I use a context here; `initialize` can be used in a couple different ways.\n",
    "with initialize(version_base=None, config_path=\"cfg\"):\n",
    "    # Use return_hydra_config=True to get the 'hydra' information in the config object\n",
    "    cfg = compose(config_name=\"config_demoF_pipeline\", return_hydra_config=True)\n",
    "    # Because initialize was used, HydraConfig was not properly configured.\n",
    "    # It requires that 'hydra' is in the config object.\n",
    "    # This is a workaround for this demo notebook and need not be used elsewhere.\n",
    "    HydraConfig.instance().set_config(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipelineContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use top-level scripts to run several stages of a pipeline. Those scripts cover multiple stages within a phase (such as all the stages for producing a simulation). Certain practicalities make it so that the stages aren't delineated as neatly as in some of my pipeline diagrams.\n",
    "\n",
    "Running multiple Executors in a row is handled by a single `PipelineContext` manager. That object has a few jobs:\n",
    "- It sets up Executors to run in order\n",
    "- It runs the Executors's `execute()` methods\n",
    "- It enables some preliminary checks for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmbml.core import PipelineContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_context = PipelineContext(cfg)\n",
    "\n",
    "pipeline_context.add_pipe(MakePSExecutor)\n",
    "pipeline_context.add_pipe(PS2MapExecutor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prerun_pipeline()` method goes through the list of Executors and initializes each one. This way I can start a pipeline and have some reassurance that there are no issues with the configurations of each. This informs what I put into the Executor's `__init__()` and what I put into `execute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_context.prerun_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I run the pipeline itself. I enclose it in a `try` block so that I'm assured that I don't lose valuable debugging information if something fails while executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CMB power spectrum written to /data/jim/CMB_Data/Datasets/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Skipping stage logs for stage MakePSExecutor.\n",
      "Power spectrum read from /data/jim/CMB_Data/Datasets/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Map written to /data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n",
      "Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pipeline_context.run_pipeline()\n",
    "except Exception as e:\n",
    "    # I typically use the logging library for these messages\n",
    "    logger.warning(\"An exception occured during the pipeline.\", exc_info=e)\n",
    "    raise e\n",
    "finally:\n",
    "    logger.info(\"Pipeline completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As much as I have error-checks and guard rails to prevent issues, those still arise and may do so silently. For this reason, I'm very careful to keep track of what was run. An early version of CMB-ML used a system called DVC to enable this, but I found it difficult to use (likely due to my own inflexibility!). To this end, I have a `LogMaker`. For each stage of the pipeline, it logs:\n",
    "- The configurations used, in both raw form and as interpolated by Hydra\n",
    "- All CMB-ML python modules imported (but not modules from external libraries)\n",
    "- The `logging` output for stages run\n",
    "\n",
    "Similar to the Namer, this runs in the background and I seldom think about it. The only place it is used directly is the top-level script. I repeat most of what I had in the PipelineContext to give the following example of it being used. \n",
    "\n",
    "**Note**: Currently this doesn't work fully in the notebook; there's a conflict in building the configuration I haven't been able to resolve. The Dataset folder will contain only configuration logs, without Python. This works well outside the notebook and a top-level script should be run if you're interested in a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CMB power spectrum written to /data/jim/CMB_Data/Datasets/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Skipping stage logs for stage MakePSExecutor.\n",
      "Power spectrum read from /data/jim/CMB_Data/Datasets/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n",
      "Map written to /data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from cmbml.core import LogMaker\n",
    "\n",
    "\n",
    "# Initialize the log maker\n",
    "log_maker = LogMaker(cfg)\n",
    "# Log the procedure to the hydra log (getting python modules)\n",
    "# I use a dummy module here. \n",
    "helper_dir = Path(os.path.abspath(os.getcwd())) / \"helpers\"\n",
    "log_maker.log_procedure_to_hydra(helper_dir / \"F_helper.py\")\n",
    "# Generally, I'd use __file__, as in the following, but that fails in notebooks.\n",
    "# log_maker.log_procedure_to_hydra(source_script=__file__)\n",
    "\n",
    "# Create the pipeline context, with the log_maker, so each stage can log\n",
    "pipeline_context = PipelineContext(cfg, log_maker)\n",
    "\n",
    "pipeline_context.add_pipe(MakePSExecutor)\n",
    "pipeline_context.add_pipe(PS2MapExecutor)\n",
    "\n",
    "pipeline_context.prerun_pipeline()\n",
    "\n",
    "try:\n",
    "    pipeline_context.run_pipeline()\n",
    "except Exception as e:\n",
    "    print(\"An exception occured during the pipeline.\", exc_info=e)\n",
    "    raise e\n",
    "finally:\n",
    "    # Add this line to the end of the pipeline, keeping logs with the dataset\n",
    "    log_maker.copy_hydra_run_to_dataset_log()\n",
    "    print(\"Pipeline completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the message \"Skipping stage logs for stage MakePSExecutor.\" If I look at the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_setup:\n",
      "  assets_out:\n",
      "    cmb_ps:\n",
      "      handler: TextPowerSpectrum\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_ps.fits'\n",
      "  dir_name: A_PS_Setup\n",
      "ps2map:\n",
      "  assets_out:\n",
      "    cmb_map:\n",
      "      handler: HealpyMap\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_map.fits'\n",
      "  assets_in:\n",
      "    cmb_ps:\n",
      "      stage: ps_setup\n",
      "  dir_name: B_CMB_Map\n",
      "  make_stage_log: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that the `ps2map` stage has `make_stage_log: true`, but `ps_setup` is missing that key:value pair. When `make_stage_log: true`, the LogMaker puts the contents of the Logs into the directory associated with the output of that particular stage. If it is missing or `false`, this does not happen and a small warning is given.\n",
    "\n",
    "For the long-running stages that produce a large volume of output, I sometimes transfer these directories and want to keep the Logs with them by default. The logs are a relatively small amount of data has been a worthwhile cost. Hoever, for many of the smaller stages (especially in the Analysis phase which outputs a bunch of figures), the Logs are unnecessary. It is for this reason that they can be disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has been an overview of how stages are assembled and run in bulk. \n",
    "\n",
    "- The `PipelineContext` class is used to collect stages of a pipeline, check that they will be properly initialized, and then run the stages in order\n",
    "- The `LogMaker` class is used to ensure reproducibility, creating an image of the codebase as it existed when the data was actually processed.\n",
    "\n",
    "These tools are important when setting up a top-level script. I hope that it explains some design decisions made in the structure of CMB-ML.\n",
    "\n",
    "In the [next notebook](./G_CMB_ML_executors.ipynb), I'll take a closer look at a few different patterns for Executors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmb-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
