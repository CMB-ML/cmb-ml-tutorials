{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMB-ML Framework: Stage Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMB-ML manages a complex pipeline that processes data across multiple stages. Each stage produces outputs that need to be tracked, reused, and processed in later stages. Without a clear framework, this can lead to disorganized code, redundant logic, and errors.\n",
    "\n",
    "The CMB-ML library provides a set of tools to manage the pipeline in a modular and scalable way.\n",
    "\n",
    "Each tool focuses on a specific task, such as handling data files, managing file paths, or defining pipeline stages. Together, they simplify building and maintaining complex workflows.\n",
    "\n",
    "This notebook will focus on the elements that occur within a single stage of the pipeline.\n",
    "\n",
    "I’ll start with the simplest components, like Assets, and build up to what's needed for a stage. Each section includes an explanation of the concept and minimal examples showing how to use it.\n",
    "\n",
    "In my initial work, I found myself hitting some stumbling blocks repeatedly:\n",
    "\n",
    "- Keeping track of inputs and outputs across stages.\n",
    "- Avoiding repetition when handling file paths and formats.\n",
    "- Scaling workflows to many simulations.\n",
    "- Supporting flexibility for different datasets and stages.\n",
    "\n",
    "By organizing these concepts into clear components, the library reduces complexity and improves reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View this notebook with [nbviewer](https://nbviewer.org/github/CMB-ML/cmb-ml/tree/main/demonstrations/E_CMB_ML_framework.ipynb#Introduction) (or in your IDE) to enable these links.\n",
    "\n",
    "This notebook first describes the running [Example](#Example) that will be used.\n",
    "\n",
    "Then I introduce these components of the CMB-ML library:\n",
    "\n",
    "- [Pipeline Config](#Pipeline-Stage-Configuration): The YAML configuration\n",
    "- [Assets](#Assets): Represent data files with methods for reading and writing.\n",
    "- [AssetHandlers](#AssetHandlers): Handle specific file formats (e.g., .fits, .npy).\n",
    "- [Namers](#Namers): Dynamically generate file paths based on the current pipeline state.\n",
    "- [Executors](#Executors): Define the logic for a single stage of the pipeline.\n",
    "\n",
    "A short [Conclusion](#Conclusion) wraps up and provides a segue to the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I consider a very simple task: converting a power spectrum to a map. For simplicity, I work on just one \"simulation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example will create a \"Dataset\" called DemoNotebook in your local_system's Dataset folder. That whole folder can be deleted when done.\n",
    "\n",
    "The following cell gives access to the configuration directory. You may need to change line 8 for your local_system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the location of the data directory\n",
    "os.environ[\"CMB_ML_DATA\"] = \"/data/jim/CMB_Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've put all the nitty-gritty code into an external module so that the notebook can be a bit cleaner. I recommend **against** looking to it for explanation. To demonstrate concepts, I've had to expose simpler classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from helpers.E_helper import cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stage Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've illustrated how Hydra configs work [previously](./A_hydra_tutorial.ipynb). In that example, the configuration had only very simple parameters. I can consider a file path to be a parameter that points to more rich data somewhere on disk. The **pipeline** portion of the configuration has the information I need to work with that data.\n",
    "\n",
    "Here's the pipeline for this simple example. I'll be \"working on\" the `ps2map` stage; and considering the `ps_setup` stage as already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_setup:\n",
      "  assets_out:\n",
      "    cmb_ps:\n",
      "      handler: TextPowerSpectrum\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_ps.fits'\n",
      "  dir_name: A_PS_Setup\n",
      "ps2map:\n",
      "  assets_out:\n",
      "    cmb_map:\n",
      "      handler: HealpyMap\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_map.fits'\n",
      "  assets_in:\n",
      "    cmb_ps:\n",
      "      stage: ps_setup\n",
      "  dir_name: B_CMB_Map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook will use this information, but there's a lot of fiddly detail. Instead, ignore the things and stuff to see this structure:\n",
    "\n",
    "```yaml\n",
    "stage1:\n",
    "    assets_out:\n",
    "        thing:\n",
    "            stuff\n",
    "    other: stuff\n",
    "stage2:\n",
    "    assets_out:\n",
    "        thing:\n",
    "            stuff\n",
    "    assets_in:\n",
    "        first_thing: stuff\n",
    "        other_thing: stuff\n",
    "    other: stuff\n",
    "```\n",
    "\n",
    "For each stage, I list (1) **what comes out**, (2) **what goes in**, and (3) **other stage details**. (The details include the output directory, the relevant splits, logging information, and (rarely) flags for particular stages. These will be described more later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, an `Asset` represents a piece of data used or created during a pipeline stage. Assets provide a consistent way to manage data, allowing you to:\n",
    "\n",
    "- Read data from files\n",
    "- Write data to files\n",
    "- Reference a file’s location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving too deep, let's see how `Asset`'s are used. Suppose we're working on an executor that makes CMB maps from power spectra. The following cell sets up the state we'd be in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.E_helper import assets_in, assets_out, make_map_from_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using automatically generated assets, I load the data, operate on it, and write the output succinctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = assets_in[\"cmb_ps\"].read()\n",
    "cmb = make_map_from_ps(ps, nside=256)\n",
    "assets_out[\"cmb_map\"].write(data=cmb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assets lets me focus on the process instead of worrying about details of file paths and formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assets are defined as parameters in the pipeline configuration YAMLs. For the assets used above, that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_setup:\n",
      "  assets_out:\n",
      "    cmb_ps:\n",
      "      handler: TextPowerSpectrum\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_ps.fits'\n",
      "  dir_name: A_PS_Setup\n",
      "ps2map:\n",
      "  assets_out:\n",
      "    cmb_map:\n",
      "      handler: HealpyMap\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_map.fits'\n",
      "  assets_in:\n",
      "    cmb_ps:\n",
      "      stage: ps_setup\n",
      "  dir_name: B_CMB_Map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Output Assets* (`assets_out`) are fully defined with both a `handler` and a `path_template`. \n",
    "- *Input Assets* (`assets_in`) reference earlier stages instead for those values.\n",
    "\n",
    "All assets have a `handler` and a `path_template`. The `handler` (an `AssetHandler`) reads or writes files and will be described in the next section. The `path_template` is a string with placeholder tags (like {root} and {stage}) used to dynamically generate the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the YAML to the values in the Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset: cmb_map\n",
      "  Handler:       HealpyMap\n",
      "  Path template: {root}/{dataset}/{stage}/cmb_dummy_map.fits\n",
      "  Path:          /data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n",
      "Asset: cmb_ps\n",
      "  Handler:       TextPowerSpectrum\n",
      "  Path template: {root}/{dataset}/{stage}/cmb_dummy_ps.fits\n",
      "  Path:          /data/jim/CMB_Data/Datasets/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n"
     ]
    }
   ],
   "source": [
    "def report_asset(asset_str, asset):\n",
    "    msg = f\"Asset: {asset_str}\\n\"\\\n",
    "          f\"  Handler:       {asset.handler.__class__.__name__}\\n\"\\\n",
    "          f\"  Path template: {asset.path_template}\\n\"\\\n",
    "          f\"  Path:          {asset.path}\"\n",
    "    print(msg)\n",
    "\n",
    "report_asset(\"cmb_map\", assets_out[\"cmb_map\"])\n",
    "report_asset(\"cmb_ps\", assets_in[\"cmb_ps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm currently working on the stage `ps2map`, which is where the two assets used above are defined. When I need to check what's going on, here's how I look at the **YAML**:\n",
    "- The output asset is straightforward. I start at `ps2map`, then look to `assets_out` and then `cmb_map`. I see that HealpyMap is defined as the handler, and the end of the path_template specifies \"cmb_dummy_map.fits\". This is confirmed in the Python output.\n",
    "- The input asset requires a bit more looking. All inputs are considered as outputs of previous stages. I start at `ps2map`, then look to `assets_in` and then `cmb_ps`. When I see `cmb_ps: {stage: ps_setup}`, I know I need to look at the `ps_setup` stage and the `assets_out`. From there I see the values reflected in the Python output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the path_template is automatically filled in to form the path. This will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety with Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason I'm clear about input versus output assets is so that I know how my data was generated. If I have an asset that handles access to a source map, I can manipulate that data all I want in RAM. However, when I try to save it I cannot accidentally overwrite the source data on disk. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not write to an input asset!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assets_in[\"cmb_ps\"].write(data=ps)\n",
    "except AttributeError:\n",
    "    print(\"Do not write to an input asset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, I cannot accidentally read an output asset. It is my hope that your code works perfectly from the very start. However, if it is anything like mine has been, you may appreciate that you cannot accidentally re-use leftovers from a previous run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AssetHandlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Asset provides a uniform interface for interacting with data. Because there are far fewer file *types* than different files, I define `AssetHandlers` to do the reading or writing for a type of data. This way, the Asset can focus on where the data will be, while the AssetHandler can focus on I/O.\n",
    "\n",
    "In some cases, especially with plain text, there are conventions within the file. In other cases, we will want the data as read to be in a particular configuration. We define different AssetHandlers for these cases as well.\n",
    "\n",
    "We can use AssetHandlers implicitly (as above in [Assets](#assets)), and this is generally the preferred way to go about it. However, I sometimes use an AssetHandler on its own. In the following example, I show how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1048.93659    1006.92305844  970.47279622]\n",
      "[1048.93659    1006.92305844  970.47279622]\n",
      "This is the same data!\n"
     ]
    }
   ],
   "source": [
    "from cmbml.core.asset_handlers import TextPowerSpectrum\n",
    "\n",
    "\n",
    "# Loading using the handler directly\n",
    "ps_path = assets_in[\"cmb_ps\"].path\n",
    "ps_handler = TextPowerSpectrum()\n",
    "ps_from_file = ps_handler.read(ps_path)\n",
    "print(ps_from_file[:3])\n",
    "\n",
    "# Loading using the asset\n",
    "ps_from_asset = assets_in[\"cmb_ps\"].read()\n",
    "print(ps_from_asset[:3])\n",
    "\n",
    "# Confirm that the data is the same:\n",
    "assert (ps_from_file == ps_from_asset).all()\n",
    "\n",
    "print(\"This is the same data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining AssetHandlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AssetHandlers are simple objects. To show this, I can extend the system by creating custom AssetHandlers for new file types. For example, suppose I want to read the power spectrum files as text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmbml.core import GenericHandler, make_directories\n",
    "\n",
    "class TextHandler(GenericHandler):\n",
    "    def read(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            res = f.read()\n",
    "        return res\n",
    "\n",
    "    def write(self, path, data):\n",
    "        # Ensure the directory exists\n",
    "        make_directories(path)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will now work as a standalone handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.048936590000000024e+03\n",
      "1.0069230584423\n"
     ]
    }
   ],
   "source": [
    "txt_handler = TextHandler()\n",
    "\n",
    "text_ps = txt_handler.read(ps_path)\n",
    "print(text_ps[:40])  # The first 40 characters of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the new handler within an Executor, I need to also register the handler. In the module with the new asset handler, I have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmbml.core.asset_handlers.asset_handler_registration import register_handler\n",
    "\n",
    "# After the class definition\n",
    "register_handler(\"TextHandler\", TextHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding AssetHandlers is not a common occurence, but this helps show that they're fairly simple. It's a convenient way to keep file-handling code out of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Namers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Namer` handles dynamic file path generation in the pipeline. Instead of hardcoding file paths, the Namer takes a path_template ({dataset}, {stage}, {sim}, or others) and fills them in based on the pipeline’s current state. This ensures that file paths remain consistent and adaptable, even across many simulations and stages.\n",
    "\n",
    "Every Asset relies on the Namer to construct its path property, making it a central piece of the pipeline’s infrastructure.\n",
    "\n",
    "You probably **do not need to create a Namer**, ever. These are handled by the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of this pipeline stage from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets_out:\n",
      "  cmb_map:\n",
      "    handler: HealpyMap\n",
      "    path_template: '{root}/{dataset}/{stage}/cmb_dummy_map.fits'\n",
      "assets_in:\n",
      "  cmb_ps:\n",
      "    stage: ps_setup\n",
      "dir_name: B_CMB_Map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline.ps2map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Namer object will take the \n",
    "\n",
    "```yaml\n",
    "        path_template: \"{root}/{dataset}/{stage}/cmb_dummy_map.fits\"\n",
    "```\n",
    "\n",
    "and convert it into an actual path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{root}/{dataset}/{stage}/cmb_dummy_map.fits\n",
      "/data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n"
     ]
    }
   ],
   "source": [
    "print(assets_out[\"cmb_map\"].path_template)\n",
    "print(assets_out[\"cmb_map\"].path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "The Namer may not seem very important right now, but it's a critical part of the CMB-ML framework. The example above was for a generic single file produced in a stage. What if we're producing noise? I'll want to produce a different noise map for each detector frequency and save each of them immediately. However, I don't want to keep track (pass around variables) of the `{root}`, `{dataset}`, and `{stage}`, even though they're needed for the filename.\n",
    "\n",
    "The structure of the Namer allows us to have defined these values ahead of time. Then, when iterating through each channel, I can simply tell the Namer what channel is being used, and it can handle the details.\n",
    "\n",
    "We're producing multiple simulations across multiple datasplits. I can tell the Namer to keep track of these individually, nesting the settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Namer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Namer is stateful, meaning it tracks the current context of the stage being run (e.g., for the current split or simulation number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'DemoNotebook',\n",
       " 'working': '',\n",
       " 'root': '/data/jim/CMB_Data/Datasets',\n",
       " 'src_root': '/data/jim/CMB_Data/Assets/'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.E_helper import name_tracker\n",
    "\n",
    "name_tracker.context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the asset, when the path property is used, it uses the Namer's `path()` method to fill in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'Key stage not found in the context. Ensure that the path_template {root}/{dataset}/{stage}/cmb_dummy_map.fits is correct in the pipeline yaml.'\n"
     ]
    }
   ],
   "source": [
    "path_template = assets_out[\"cmb_map\"].path_template\n",
    "try:\n",
    "    name_tracker.path(path_template)\n",
    "except KeyError as e:\n",
    "    print('KeyError:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the template to the current context of the Namer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{root}/{dataset}/{stage}/cmb_dummy_map.fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'DemoNotebook',\n",
       " 'working': '',\n",
       " 'root': '/data/jim/CMB_Data/Datasets',\n",
       " 'src_root': '/data/jim/CMB_Data/Assets/'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(path_template)\n",
    "name_tracker.context\n",
    "# Notice \"stage\" is in the path_template, but there's no matching key in the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we don't have the stage set. Normally, the asset within an Executor will handle this. In this case, I set it temporarily. I try to always use the context manager to do this. Outside the `with` block, the Namer's context reverts. This ensures that I can't forget to change the context (e.g., a simulation number or current split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n"
     ]
    }
   ],
   "source": [
    "with name_tracker.set_context(\"stage\", \"B_CMB_Map\"):\n",
    "    print(name_tracker.path(path_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At times, I'll want to set multiple things in the context simultaneously. This is especially common when setting up for parallelized runs. Instead of multiple nested `with` blocks, I use the `set_contexts()` (note set_context***s***(), not set_context()) function instead, with the following pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'DemoNotebook', 'working': '', 'root': '/data/jim/CMB_Data/Datasets', 'src_root': '/data/jim/CMB_Data/Assets/'}\n",
      "{'dataset': 'DemoNotebook', 'working': '', 'root': '/data/jim/CMB_Data/Datasets', 'src_root': '/data/jim/CMB_Data/Assets/', 'freq': 100, 'epoch': 50}\n",
      "{'dataset': 'DemoNotebook', 'working': '', 'root': '/data/jim/CMB_Data/Datasets', 'src_root': '/data/jim/CMB_Data/Assets/'}\n"
     ]
    }
   ],
   "source": [
    "# Show the initial context\n",
    "print(name_tracker.context)  # no \"freq\" or \"epoch\"\n",
    "\n",
    "# Show the context within the context manager\n",
    "new_context = dict(freq=100, epoch=50)\n",
    "with name_tracker.set_contexts(new_context):\n",
    "    print(name_tracker.context)  # includes \"freq\" and \"epoch\"\n",
    "\n",
    "# Show the context after the context manager\n",
    "print(name_tracker.context)  # no \"freq\" or \"epoch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All assets have access to the same Namer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No assertion errors: all objects are the same\n"
     ]
    }
   ],
   "source": [
    "# We know name_tracker is the same for all assets; it was created in the same context\n",
    "assert name_tracker == assets_out[\"cmb_map\"].name_tracker\n",
    "assert name_tracker == assets_in[\"cmb_ps\"].name_tracker\n",
    "assert assets_out[\"cmb_map\"].name_tracker == assets_in[\"cmb_ps\"].name_tracker\n",
    "\n",
    "# Testing equality of objects shows they're the *same* object, not just equivalent\n",
    "print(\"No assertion errors: all objects are the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the Namer will take care of the paths, and that the AssetHandler will take care of reading and writing data, I can now move on to defining the stage itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've now got the essential components of an `Executor` and can describe it as a whole.\n",
    "\n",
    "An Executor represents a single stage in the pipeline. Its primary role is to process input data (via Assets) and produce output data. Executors are modular, meaning each stage is implemented as its own class, making the pipeline easy to extend, debug, and maintain.\n",
    "\n",
    "At a high level, an Executor:\n",
    "\n",
    "- Initializes input and output Assets.\n",
    "- Defines the logic for processing data.\n",
    "- Runs the process for all relevant simulations and splits.\n",
    "\n",
    "We'll again start with an example, set up elsewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.E_helper import helper, InitPS2MapExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, within the config for the pipeline, the `ps2map` stage has been set up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets_out:\n",
      "  cmb_map:\n",
      "    handler: HealpyMap\n",
      "    path_template: '{root}/{dataset}/{stage}/cmb_dummy_map.fits'\n",
      "assets_in:\n",
      "  cmb_ps:\n",
      "    stage: ps_setup\n",
      "dir_name: B_CMB_Map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline.ps2map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This executor has the correct assets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      "  cmb_map\n",
      "    Handler:  HealpyMap\n",
      "    Template: {root}/{dataset}/{stage}/cmb_dummy_map.fits\n",
      "    Path:     /data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n",
      "Inputs:\n",
      "  cmb_ps\n",
      "    Handler:  TextPowerSpectrum\n",
      "    Template: {root}/{dataset}/{stage}/cmb_dummy_ps.fits\n",
      "    Path:     /data/jim/CMB_Data/Datasets/DemoNotebook/A_PS_Setup/cmb_dummy_ps.fits\n"
     ]
    }
   ],
   "source": [
    "def display_asset_list(asset_list):\n",
    "    for asset_key, asset_val in asset_list.items():\n",
    "        print(f\"  {asset_key}\")\n",
    "        print(f\"    Handler:  {asset_val.handler.__class__.__name__}\")\n",
    "        print(f\"    Template: {asset_val.path_template}\")\n",
    "        print(f\"    Path:     {asset_val.path}\")\n",
    "\n",
    "print(\"Outputs:\")\n",
    "display_asset_list(helper.assets_out)\n",
    "print(\"Inputs:\")\n",
    "display_asset_list(helper.assets_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it has the following `.execute()` method:\n",
    "\n",
    "```python\n",
    "    def execute(self):\n",
    "        print(\"This is an example helper\")\n",
    "        return\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example executor.\n"
     ]
    }
   ],
   "source": [
    "helper.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trivial method runs, but it isn't a good example of using all the tools described so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've got the pipeline defined in the YAML. I know what Assets I need, I have the AssetHandlers working, and I know how I can use the Namer to manage paths for me. Now I'll rebuild it. I usually have to review my pipeline configuration yaml at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_setup:\n",
      "  assets_out:\n",
      "    cmb_ps:\n",
      "      handler: TextPowerSpectrum\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_ps.fits'\n",
      "  dir_name: A_PS_Setup\n",
      "ps2map:\n",
      "  assets_out:\n",
      "    cmb_map:\n",
      "      handler: HealpyMap\n",
      "      path_template: '{root}/{dataset}/{stage}/cmb_dummy_map.fits'\n",
      "  assets_in:\n",
      "    cmb_ps:\n",
      "      stage: ps_setup\n",
      "  dir_name: B_CMB_Map\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I write out the `__init__()` method. I specify the \"stage_str\", which is the name of the stage as it appears in the YAML file. Then I define instance variables for each of my assets. I could alsoo define other variables, based on parameters in the `cfg`. I take great pains to avoid defining `self.cfg = cfg`, should the inclination arise.\n",
    "\n",
    "Last, I define the `execute()` method. It must have this name, and it must not take any parameters. In this case, I can use the logic from [Assets Overview](#Overview) (repeated here):\n",
    "\n",
    "```python\n",
    "ps = assets_in[\"cmb_ps\"].read()\n",
    "cmb = make_map_from_ps(ps, nside=256)\n",
    "assets_out[\"cmb_map\"].write(data=cmb)\n",
    "```\n",
    "\n",
    "Other things may be included as well, but this suffices for the current example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "from cmbml.core import BaseStageExecutor\n",
    "\n",
    "class PS2MapExecutor(BaseStageExecutor):\n",
    "    def __init__(self, cfg) -> None:\n",
    "        # I call the base class's __init__ method\n",
    "        # The stage_str matches the key in the pipeline config\n",
    "        super().__init__(cfg, stage_str=\"ps2map\")\n",
    "\n",
    "        # The base class will have generated assets_in and assets_out\n",
    "        # I give myself more descriptive names.\n",
    "        # I use \"in_\" or \"out_\" prefixes and \"_asset\" suffix for clarity\n",
    "        self.out_map_asset = self.assets_out[\"cmb_map\"]\n",
    "        self.in_ps_asset = self.assets_in[\"cmb_ps\"]\n",
    "\n",
    "    def execute(self) -> None:\n",
    "        # Use the input asset to read the power spectrum\n",
    "        ps = self.in_ps_asset.read()\n",
    "        print(f\"Power spectrum read from {self.out_map_asset.path}\")\n",
    "        \n",
    "        # Perform the operation\n",
    "        cmb = hp.synfast(ps, nside=256)\n",
    "\n",
    "        # Save the resulting map using the output asset\n",
    "        self.out_map_asset.write(data=cmb)\n",
    "        print(f\"Map written to {self.out_map_asset.path}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power spectrum read from /data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n",
      "Map written to /data/jim/CMB_Data/Datasets/DemoNotebook/B_CMB_Map/cmb_dummy_map.fits\n"
     ]
    }
   ],
   "source": [
    "ps2map_executor = PS2MapExecutor(cfg)\n",
    "ps2map_executor.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of all the work in the other classes above, this is simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there's a lot more that can be done. This is a very simple example that doesn't go into processing across multiple splits or simulations. In those cases I have a few different structures I can use. Often, there are also several steps of processing to apply within a stage. Those will be addressed [elsewhere](./G_CMB_ML_executors.ipynb). First, though, I'll look at a pipeline as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've set the groundwork for the main design patterns used as portions of a pipeline stage. You've seen how these different classes interact. \n",
    "\n",
    "- The pipeline configuration YAML defines *how* and from *where* data is read\n",
    "- `Asset`s are an easy interface to that data\n",
    "- `AssetHandler`s do the actual reading or writing of the data\n",
    "- The `Namer` simplifies working with the paths to data\n",
    "- `Executor`s automatically set up those classes and then include the business logic for a stage\n",
    "\n",
    "In the [next notebook](F_CMB_ML_pipeline.ipynb), we'll look at how stages are put together to form pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmb-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
